# Pipeline

This directory contains all the necessary code and documentation for the Extract Transform Load (ETL) pipeline used to collect, clean and upload the data collected from the live data stream(s).

## Requirements ğŸ“‹

To run this script, you will need the following:
- **Python**: Version 3.10
- `pytest`: For running unit tests
- `pytest-cov`: For measuring test coverage
- `requests`: For making HTTP requests
- `pandas`: For data manipulation and analysis
- `python-dotenv`: For loading environment variables from a `.env` file


To install these dependencies, use the following command:

```zsh
pip3 install -r requirements.txt
```

## Files Explained ğŸ—‚ï¸


## Secrets Management ğŸ•µğŸ½â€â™‚ï¸
Before running the script, you need to set up your AWS credentials. Create a new file called `.env` in the `pipeline` directory and add the following lines, with your actual AWS keys and database details:

| Variable         | Description                                      |
|------------------|--------------------------------------------------|
| DB_HOST          | The hostname or IP address of the database.      |
| DB_PORT          | The port number for the database connection.     |
| DB_PASSWORD      | The password for the database user.              |
| DB_USER          | The username for the database.                   |
| DB_NAME          | The name of the database.                        |
| SCHEMA_NAME      | The name of the database schema.                 |
